{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*-coding:utf-8\n",
    "'''\n",
    "Created on Fri Dec 1 22:22:35 2017\n",
    "\n",
    "@author: Ray\n",
    "\n",
    "'''\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from glob import glob\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib import pyplot\n",
    "import utils # made by author for efficiently dealing with data\n",
    "\n",
    "seed = 72\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep top imp\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:06<02:03,  6.47s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:13<01:57,  6.51s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:19<01:48,  6.35s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:25<01:40,  6.26s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:32<01:36,  6.44s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:39<01:31,  6.55s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:46<01:26,  6.65s/it]\u001b[A\n",
      " 40%|████      | 8/20 [00:52<01:19,  6.62s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:59<01:12,  6.61s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [01:06<01:06,  6.64s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [01:13<00:59,  6.66s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [01:19<00:52,  6.61s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [01:25<00:46,  6.59s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [01:31<00:39,  6.55s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [01:37<00:32,  6.53s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [01:44<00:26,  6.51s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [01:50<00:19,  6.48s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [01:55<00:12,  6.44s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [02:02<00:06,  6.45s/it]\u001b[A\n",
      "100%|██████████| 20/20 [02:09<00:00,  6.46s/it]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainW-0.shape:(881896, 50)\n",
      "\n",
      "keep top imp\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:06<02:08,  6.77s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:13<02:03,  6.83s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:20<01:54,  6.75s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:26<01:46,  6.66s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:33<01:40,  6.67s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:39<01:32,  6.58s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:45<01:24,  6.52s/it]\u001b[A\n",
      " 40%|████      | 8/20 [00:52<01:18,  6.51s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:58<01:11,  6.54s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [01:05<01:05,  6.56s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [01:12<00:59,  6.61s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [01:19<00:53,  6.63s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [01:25<00:46,  6.62s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [01:32<00:39,  6.59s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [01:39<00:33,  6.67s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [01:47<00:26,  6.71s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [01:55<00:20,  6.78s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [02:02<00:13,  6.78s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [02:09<00:06,  6.79s/it]\u001b[A\n",
      "100%|██████████| 20/20 [02:15<00:00,  6.80s/it]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainW-1.shape:(884309, 50)\n",
      "\n",
      "keep top imp\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:08<02:35,  8.18s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:17<02:34,  8.60s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:24<02:20,  8.24s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:33<02:15,  8.49s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:45<02:16,  9.10s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:53<02:04,  8.88s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:59<01:51,  8.57s/it]\u001b[A\n",
      " 40%|████      | 8/20 [01:06<01:39,  8.33s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [01:16<01:33,  8.48s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [01:23<01:23,  8.32s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [01:29<01:13,  8.15s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [01:39<01:06,  8.29s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [01:48<00:58,  8.38s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [01:57<00:50,  8.38s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [02:04<00:41,  8.33s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [02:12<00:33,  8.27s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [02:19<00:24,  8.19s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [02:26<00:16,  8.13s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [02:32<00:08,  8.05s/it]\u001b[A\n",
      "100%|██████████| 20/20 [02:39<00:00,  7.99s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainW-2.shape:(905872, 50)\n",
      "\n",
      "per_churned_in_train_0 0.0587484238504\n",
      "n_churned 10042\n",
      "per_churned_in_train_1 0.0554877139084\n",
      "per_churned_in_train_2 0.0554877139084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################\n",
    "#load dataset\n",
    "##########################################\n",
    "\n",
    "# load dataset\n",
    "file_name = '../output/model/xgb_feature_tuning_seed_72.model'\n",
    "train_0 = utils.load_pred_feature('trainW-0', keep_all = False, model_file_name = file_name, n_top_features = 48)\n",
    "train_1 = utils.load_pred_feature('trainW-1', keep_all = False, model_file_name = file_name, n_top_features = 48)\n",
    "train_2 = utils.load_pred_feature('trainW-2', keep_all = False, model_file_name = file_name, n_top_features = 48)\n",
    "# make data augmentation having same label distribution with training set provided by the kkbox\n",
    "per_churned_in_train_0 = train_0[['is_churn']].describe().ix['mean'][0] \n",
    "n_churned = train_1[train_1.is_churn == 0].shape[0] * per_churned_in_train_0\n",
    "print('per_churned_in_train_0', per_churned_in_train_0)\n",
    "print('n_churned', int(n_churned))\n",
    "train_1 = pd.concat([train_1[train_1.is_churn == 0],\n",
    "                train_1[train_1.is_churn == 1].sample(n = int(n_churned), random_state = seed)\n",
    "               ], ignore_index=True)\n",
    "per_churned_in_train_1 = train_1[['is_churn']].describe().ix['mean'][0] \n",
    "print('per_churned_in_train_1', per_churned_in_train_1)\n",
    "train_2 = pd.concat([train_2[train_2.is_churn == 0],\n",
    "                train_2[train_2.is_churn == 1].sample(n = int(n_churned), random_state = seed)\n",
    "               ], ignore_index=True)\n",
    "per_churned_in_train_2 = train_1[['is_churn']].describe().ix['mean'][0] \n",
    "print('per_churned_in_train_2', per_churned_in_train_2)\n",
    "train = pd.concat([train_0, train_1, train_2], ignore_index=True)\n",
    "\n",
    "# reduce memory in python\n",
    "del train_0, train_1, train_2, per_churned_in_train_0, per_churned_in_train_1, per_churned_in_train_2, n_churned,\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepartion of training set is done\n",
      "(1130966, 48)\n",
      "(1130966,)\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# prepare training data\n",
    "#==============================================================================\n",
    "Y_train = train['is_churn'] \n",
    "X_train = train.drop('is_churn', axis=1)\n",
    "del train\n",
    "\n",
    "print ('prepartion of training set is done')\n",
    "##########################################\n",
    "#Step 1: Fix learning rate and number of estimators for tuning tree-based parameters\n",
    "##########################################\n",
    "from sklearn.learning_curve import validation_curve # Determine training and test scores for varying parameter values.\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from scipy.sparse import vstack\n",
    "'''\n",
    "Operation procedure:\n",
    "1. fixed learning_rate =0.1\n",
    "2. using cross-validation fisrt to find optimal number of trees, then use the fixed n_estimators\n",
    "    the following tree-based parameters are default value. Later on, we need to tune this parameters.\n",
    "    -max_depth = 6\n",
    "    -min_child_weight = 1\n",
    "    -gamma = 0\n",
    "    -subsample, colsample_bytree = 0.8 \n",
    "    -scale_pos_weight = 1\n",
    "    \n",
    "'''\n",
    "\n",
    "default_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma' : 0,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'silent': 1.0\n",
    "}\n",
    "# prepare X,y\n",
    "print (X_train.values[:,1:].shape)\n",
    "print (Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to find optimum n_estimators that we need to tune our tree-based paras\n",
      "n_estimators_range [  50  155  261  366  472  577  683  788  894 1000]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-54525b81ebee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtest_scores_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m# plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation Curve with XGBoost (eta = 0.1)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#We will divide into 10 stratified folds (the same distibution of labels in each fold) for testing\n",
    "cv = StratifiedKFold(Y_train, n_folds=3, shuffle=True, random_state=seed)\n",
    "\n",
    "#Return evenly spaced numbers over a specified interval.\n",
    "n_estimators_range = np.linspace(start = 50, stop = 1000, num = 10).astype('int') \n",
    "\n",
    "print ('Starting to find optimum n_estimators that we need to tune our tree-based paras')\n",
    "# tuning n_estimators\n",
    "train_scores, test_scores = validation_curve(\n",
    "    XGBClassifier(**default_params),\n",
    "    X_train.values[:,1:], \n",
    "    Y_train.values,\n",
    "    param_name = 'n_estimators',\n",
    "    param_range = n_estimators_range,\n",
    "    cv=cv,\n",
    "    scoring='neg_log_loss' # Due to some problems, the bigger the score, the better the performance\n",
    ")\n",
    "print ('n_estimators_range', n_estimators_range)\n",
    "# for following plot\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2YAAAIaCAYAAABLQl7wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJUmMGM0mSJElqzGAmSZIkSY39f1SvqSNVl0yDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f4b5588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation result (-0.13) obtained for 577 trees\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 6), dpi=100)\n",
    "# plot\n",
    "plt.title(\"Validation Curve with XGBoost (eta = 0.1)\")\n",
    "plt.xlabel(\"number of trees\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(-0.3, 0)\n",
    "\n",
    "plt.plot(n_estimators_range,\n",
    "             train_scores_mean,\n",
    "             label=\"Training score\",\n",
    "             color=\"r\")\n",
    "\n",
    "plt.plot(n_estimators_range,\n",
    "             test_scores_mean, \n",
    "             label=\"Cross-validation score\",\n",
    "             color=\"g\")\n",
    "\n",
    "plt.fill_between(n_estimators_range, \n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, \n",
    "                 alpha=0.2, color=\"r\")\n",
    "\n",
    "plt.fill_between(n_estimators_range,\n",
    "                 test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std,\n",
    "                 alpha=0.2, color=\"g\")\n",
    "\n",
    "plt.axhline(y=1, color='k', ls='dashed')\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "best_i = np.argmax(test_scores_mean) # best index\n",
    "\n",
    "'''\n",
    "The model need to be trained 10(parameters_range) * 3 (K-fold = 10) : 30 \n",
    "Notice: 先初步找到range of n_estimators, In the end of hyperparameters tuning, \n",
    "we should lower the learning rate and add more trees. \n",
    "'''\n",
    "\n",
    "print(\"Best cross-validation result ({0:.2f}) obtained for {1} trees\".format(test_scores_mean[best_i], n_estimators_range[best_i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 7, 9]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(3,10,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results [mean: -0.13348, std: 0.00071, params: {'max_depth': 3, 'min_child_weight': 1}, mean: -0.13355, std: 0.00064, params: {'max_depth': 3, 'min_child_weight': 3}, mean: -0.13350, std: 0.00065, params: {'max_depth': 3, 'min_child_weight': 5}, mean: -0.12897, std: 0.00063, params: {'max_depth': 5, 'min_child_weight': 1}, mean: -0.12908, std: 0.00061, params: {'max_depth': 5, 'min_child_weight': 3}, mean: -0.12910, std: 0.00063, params: {'max_depth': 5, 'min_child_weight': 5}, mean: -0.12823, std: 0.00059, params: {'max_depth': 7, 'min_child_weight': 1}, mean: -0.12817, std: 0.00053, params: {'max_depth': 7, 'min_child_weight': 3}, mean: -0.12811, std: 0.00058, params: {'max_depth': 7, 'min_child_weight': 5}, mean: -0.13003, std: 0.00066, params: {'max_depth': 9, 'min_child_weight': 1}, mean: -0.12932, std: 0.00061, params: {'max_depth': 9, 'min_child_weight': 3}, mean: -0.12898, std: 0.00059, params: {'max_depth': 9, 'min_child_weight': 5}]\n",
      "best paras {'max_depth': 7, 'min_child_weight': 5}\n",
      "best_score_ -0.12810652765349884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHere we trained 12(parameters combinations) * 5 (K-fold = 3) : 50 models\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "############################\n",
    "#1. Start with wider range \n",
    "############################\n",
    "param_test1 = {\n",
    " 'max_depth':[i for i in range(3,10,2)], # star from 3 and end at 9 with step 2. For example, [3,5,7,9]\n",
    " 'min_child_weight':[i for i in range(1,6,2)] # [1,3,5]\n",
    "}\n",
    "fixed_estimator = 577\n",
    "\n",
    "cv = StratifiedKFold(Y_train, n_folds=3, shuffle=True, random_state=72)\n",
    "'''\n",
    "Here we trained 12(parameters combinations) * 3 (K-fold = 3) : 36 models\n",
    "\n",
    "'''\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, \n",
    "\tn_estimators = fixed_estimator, \n",
    "\tmax_depth=5,\n",
    " \tmin_child_weight=1, \n",
    " \tgamma=0, \n",
    " \tsubsample=0.8, \n",
    " \tcolsample_bytree=0.8,\n",
    " \tobjective= 'binary:logistic', \n",
    " \tnthread=4, \n",
    " \tscale_pos_weight=1, \n",
    " \tseed=seed), \n",
    " \tparam_grid = param_test1, \n",
    " \tscoring='neg_log_loss',\n",
    " \tn_jobs=4,\n",
    " \tcv=cv)\n",
    "# running\n",
    "gsearch1.fit(X_train, Y_train)\n",
    "\n",
    "print ('results',gsearch1.grid_scores_)\n",
    "print ('best paras',gsearch1.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_['max_depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_['min_child_weight'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results [mean: -0.12826, std: 0.00055, params: {'max_depth': 6, 'min_child_weight': 4}, mean: -0.12830, std: 0.00059, params: {'max_depth': 6, 'min_child_weight': 5}, mean: -0.12829, std: 0.00063, params: {'max_depth': 6, 'min_child_weight': 6}, mean: -0.12808, std: 0.00057, params: {'max_depth': 7, 'min_child_weight': 4}, mean: -0.12811, std: 0.00058, params: {'max_depth': 7, 'min_child_weight': 5}, mean: -0.12813, std: 0.00065, params: {'max_depth': 7, 'min_child_weight': 6}, mean: -0.12842, std: 0.00060, params: {'max_depth': 8, 'min_child_weight': 4}, mean: -0.12838, std: 0.00064, params: {'max_depth': 8, 'min_child_weight': 5}, mean: -0.12836, std: 0.00057, params: {'max_depth': 8, 'min_child_weight': 6}]\n",
      "best paras {'max_depth': 7, 'min_child_weight': 4}\n",
      "best_score_ -0.12807966623946077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nHere we trained 9(parameters combinations) * 3 (K-fold = 3) : 27 models\\n\\nNotice that:\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################\n",
    "#2. Lets go one step deeper(interval = 1) and look for optimum values.\n",
    "############################\n",
    "\n",
    "param_test2 = {\n",
    " 'max_depth':[gsearch1.best_params_['max_depth'] - 1, gsearch1.best_params_['max_depth'], gsearch1.best_params_['max_depth'] + 1],\n",
    " 'min_child_weight':[gsearch1.best_params_['min_child_weight'] - 1, gsearch1.best_params_['min_child_weight'], gsearch1.best_params_['min_child_weight'] + 1]\n",
    "}\n",
    "'''\n",
    "\n",
    "Here we trained 9(parameters combinations) * 3 (K-fold = 3) : 27 models\n",
    "\n",
    "\n",
    "'''\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, \n",
    "\tn_estimators = fixed_estimator, \n",
    "\tmax_depth=5,\n",
    " \tmin_child_weight=1, \n",
    " \tgamma=0, \n",
    " \tsubsample=0.8, \n",
    " \tcolsample_bytree=0.8,\n",
    " \tobjective= 'binary:logistic', \n",
    " \tnthread=4, \n",
    " \tscale_pos_weight=1, \n",
    " \tseed=seed), \n",
    " \tparam_grid = param_test2, \n",
    " \tscoring='neg_log_loss',\n",
    " \tn_jobs=4,\n",
    " \tcv=cv)\n",
    "# running\n",
    "gsearch2.fit(X_train, Y_train)\n",
    "\n",
    "print ('results',gsearch2.grid_scores_)\n",
    "print ('best paras',gsearch2.best_params_)\n",
    "print ('best_score_',gsearch2.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "g1: best_score_ -0.12810652765349884\n",
    "\n",
    "g2: best_score_ -0.12807966623946077 (Compared to g1, model inded get slightly inprovement)\n",
    "\n",
    "'''\n",
    "\n",
    "gsearch2.best_params_['min_child_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results [mean: -0.12808, std: 0.00057, params: {'gamma': 0.0}, mean: -0.12812, std: 0.00063, params: {'gamma': 0.1}, mean: -0.12822, std: 0.00058, params: {'gamma': 0.2}, mean: -0.12813, std: 0.00063, params: {'gamma': 0.3}, mean: -0.12815, std: 0.00062, params: {'gamma': 0.4}]\n",
      "best paras {'gamma': 0.0}\n",
      "best_score_ -0.12807966623946077\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "#Step 2: Tune gamma\n",
    "##########################################\n",
    "'''\n",
    "gamma: Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "The values depend on the loss function and should be tuned.\n",
    "\n",
    "Higher values make the algorithm conservative.\n",
    "'''\n",
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)] # [0, 0.1, 0.2, 0.3, 0.4]\n",
    "}\n",
    "'''\n",
    "\n",
    "Here we trained 5(parameters combinations) * 3 (K-fold = 3) : 15 models\n",
    "\n",
    "\n",
    "'''\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, \n",
    "\tn_estimators = fixed_estimator, \n",
    "\tmax_depth = gsearch2.best_params_['max_depth'],\n",
    " \tmin_child_weight= gsearch2.best_params_['min_child_weight'], \n",
    " \tgamma=0, \n",
    " \tsubsample=0.8, \n",
    " \tcolsample_bytree=0.8,\n",
    " \tobjective= 'binary:logistic', \n",
    " \tnthread=4, \n",
    " \tscale_pos_weight=1, \n",
    " \tseed=seed), \n",
    " \tparam_grid = param_test3, \n",
    " \tscoring='neg_log_loss',\n",
    " \tn_jobs=4,\n",
    " \tcv=cv)\n",
    "# running\n",
    "gsearch3.fit(X_train, Y_train)\n",
    "\n",
    "print ('results',gsearch3.grid_scores_)\n",
    "print ('best paras',gsearch3.best_params_)\n",
    "print ('best_score_',gsearch3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch3.best_params_['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results [mean: -0.12872, std: 0.00060, params: {'colsample_bytree': 0.6, 'subsample': 0.6}, mean: -0.12838, std: 0.00062, params: {'colsample_bytree': 0.6, 'subsample': 0.7}, mean: -0.12803, std: 0.00066, params: {'colsample_bytree': 0.6, 'subsample': 0.8}, mean: -0.12787, std: 0.00062, params: {'colsample_bytree': 0.6, 'subsample': 0.9}, mean: -0.12862, std: 0.00053, params: {'colsample_bytree': 0.7, 'subsample': 0.6}, mean: -0.12834, std: 0.00063, params: {'colsample_bytree': 0.7, 'subsample': 0.7}, mean: -0.12810, std: 0.00065, params: {'colsample_bytree': 0.7, 'subsample': 0.8}, mean: -0.12787, std: 0.00069, params: {'colsample_bytree': 0.7, 'subsample': 0.9}, mean: -0.12862, std: 0.00063, params: {'colsample_bytree': 0.8, 'subsample': 0.6}, mean: -0.12842, std: 0.00065, params: {'colsample_bytree': 0.8, 'subsample': 0.7}, mean: -0.12808, std: 0.00057, params: {'colsample_bytree': 0.8, 'subsample': 0.8}, mean: -0.12789, std: 0.00064, params: {'colsample_bytree': 0.8, 'subsample': 0.9}, mean: -0.12865, std: 0.00065, params: {'colsample_bytree': 0.9, 'subsample': 0.6}, mean: -0.12852, std: 0.00060, params: {'colsample_bytree': 0.9, 'subsample': 0.7}, mean: -0.12817, std: 0.00071, params: {'colsample_bytree': 0.9, 'subsample': 0.8}, mean: -0.12790, std: 0.00059, params: {'colsample_bytree': 0.9, 'subsample': 0.9}]\n",
      "best paras {'colsample_bytree': 0.7, 'subsample': 0.9}\n",
      "best_score_ -0.12786623104386283\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "#Step 4: Tune subsample and colsample_bytree\n",
    "##########################################\n",
    "\n",
    "'''\n",
    "The reason we tune the two parameters is to add randomness for making training robust to noise\n",
    "\n",
    "subsample: subsample ratio of the training instance.\n",
    "colsample_bytree: subsample ratio of columns for each split, in each level.\n",
    "\n",
    "We'll tune this in 2 stages.\n",
    "1. To begin with, take values 0.6,0.7,0.8,0.9 for both to start with.\n",
    "2. Secondly, Now we should try values in 0.05 interval around optimum values we found above.\n",
    "'''\n",
    "\n",
    "############################\n",
    "#1. Start with wider range \n",
    "############################\n",
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)], # [0,6, 0,7, 0.8, 0.9]\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)] # [0,6, 0,7, 0.8, 0.9]\n",
    "}\n",
    "'''\n",
    "Here we trained 16(parameters combinations) * 3 (K-fold = 3) : 48 models\n",
    "\n",
    "'''\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, \n",
    "\tn_estimators = fixed_estimator, \n",
    "\tmax_depth= gsearch2.best_params_['max_depth'],\n",
    " \tmin_child_weight= gsearch2.best_params_['min_child_weight'], \n",
    " \tgamma= gsearch3.best_params_['gamma'], \n",
    " \tsubsample=0.8, \n",
    " \tcolsample_bytree=0.8,\n",
    " \tobjective= 'binary:logistic', \n",
    " \tnthread=4, \n",
    " \tscale_pos_weight=1, \n",
    " \tseed=seed), \n",
    " \tparam_grid = param_test4, \n",
    " \tscoring='neg_log_loss',\n",
    " \tn_jobs=4,\n",
    " \tcv=cv)\n",
    "# running\n",
    "gsearch4.fit(X_train, Y_train)\n",
    "\n",
    "print ('results',gsearch4.grid_scores_)\n",
    "print ('best paras',gsearch4.best_params_)\n",
    "print ('best_score_',gsearch4.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.85, 0.9, 0.9500000000000001]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[gsearch4.best_params_['subsample']-0.05 ,gsearch4.best_params_['subsample'], gsearch4.best_params_['subsample'] + 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results [mean: -0.12810, std: 0.00070, params: {'colsample_bytree': 0.6499999999999999, 'subsample': 0.85}, mean: -0.12786, std: 0.00067, params: {'colsample_bytree': 0.6499999999999999, 'subsample': 0.9}, mean: -0.12784, std: 0.00063, params: {'colsample_bytree': 0.6499999999999999, 'subsample': 0.9500000000000001}, mean: -0.12803, std: 0.00064, params: {'colsample_bytree': 0.7, 'subsample': 0.85}, mean: -0.12787, std: 0.00069, params: {'colsample_bytree': 0.7, 'subsample': 0.9}, mean: -0.12788, std: 0.00063, params: {'colsample_bytree': 0.7, 'subsample': 0.9500000000000001}, mean: -0.12804, std: 0.00067, params: {'colsample_bytree': 0.75, 'subsample': 0.85}, mean: -0.12788, std: 0.00067, params: {'colsample_bytree': 0.75, 'subsample': 0.9}, mean: -0.12779, std: 0.00062, params: {'colsample_bytree': 0.75, 'subsample': 0.9500000000000001}]\n",
      "best paras {'colsample_bytree': 0.75, 'subsample': 0.9500000000000001}\n",
      "best_score_ -0.1277880892465591\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "#2. Lets go one step deeper(interval = 0.05) and look for optimum values.\n",
    "############################\n",
    "param_test5 = {\n",
    " 'subsample':[gsearch4.best_params_['subsample']-0.05 ,gsearch4.best_params_['subsample'], gsearch4.best_params_['subsample'] + 0.05], # [0.85, 0.9, 0.95] \n",
    " 'colsample_bytree':[gsearch4.best_params_['colsample_bytree']-0.05 ,gsearch4.best_params_['colsample_bytree'], gsearch4.best_params_['colsample_bytree'] + 0.05] # # [0.65, 0.7, 0.75] \n",
    "}\n",
    "\n",
    "'''\n",
    "Here we trained 9(parameters combinations) * 3 (K-fold = 3) : 9 models\n",
    "\n",
    "'''\n",
    "gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, \n",
    "\tn_estimators = fixed_estimator, \n",
    "\tmax_depth= gsearch2.best_params_['max_depth'],\n",
    " \tmin_child_weight= gsearch2.best_params_['min_child_weight'], \n",
    " \tgamma= gsearch3.best_params_['gamma'], \n",
    " \tsubsample=0.8, \n",
    " \tcolsample_bytree= 0.8,\n",
    " \tobjective= 'binary:logistic', \n",
    " \tnthread=4, \n",
    " \tscale_pos_weight=1, \n",
    " \tseed=seed), \n",
    " \tparam_grid = param_test5, \n",
    " \tscoring='neg_log_loss',\n",
    " \tn_jobs=4,\n",
    " \tcv=cv)\n",
    "# running\n",
    "gsearch5.fit(X_train, Y_train)\n",
    "\n",
    "print ('results',gsearch5.grid_scores_)\n",
    "print ('best paras',gsearch5.best_params_)\n",
    "print ('best_score_',gsearch5.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch5.best_params_['colsample_bytree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results [mean: -0.12780, std: 0.00069, params: {'subsample': 1.0}]\n",
      "best paras {'subsample': 1.0}\n",
      "best_score_ -0.1277976053088933\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "#2. Lets go one step deeper(interval = 0.05) and look for optimum values.\n",
    "############################\n",
    "param_test6 = {\n",
    " 'subsample':[1.0]\n",
    "}\n",
    "'''\n",
    "Here we trained 1(parameters combinations) * 3 (K-fold = 3) : 3 models\n",
    "\n",
    "'''\n",
    "gsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, \n",
    "\tn_estimators = fixed_estimator, \n",
    "\tmax_depth= gsearch2.best_params_['max_depth'],\n",
    " \tmin_child_weight= gsearch2.best_params_['min_child_weight'], \n",
    " \tgamma= gsearch3.best_params_['gamma'], \n",
    " \tsubsample=0.8, \n",
    " \tcolsample_bytree= gsearch5.best_params_['colsample_bytree'],\n",
    " \tobjective= 'binary:logistic', \n",
    " \tnthread=4, \n",
    " \tscale_pos_weight=1, \n",
    " \tseed=seed), \n",
    " \tparam_grid = param_test6, \n",
    " \tscoring='neg_log_loss',\n",
    " \tn_jobs=4,\n",
    " \tcv=cv)\n",
    "# running\n",
    "gsearch6.fit(X_train, Y_train)\n",
    "\n",
    "print ('results',gsearch6.grid_scores_)\n",
    "print ('best paras',gsearch6.best_params_)\n",
    "print ('best_score_',gsearch6.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.1277880892465591 > -0.1277976053088933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results [mean: -0.12792, std: 0.00075, params: {'reg_lambda': 1e-05}, mean: -0.12784, std: 0.00065, params: {'reg_lambda': 0.01}, mean: -0.12789, std: 0.00063, params: {'reg_lambda': 0.1}, mean: -0.12779, std: 0.00062, params: {'reg_lambda': 1}, mean: -0.12811, std: 0.00054, params: {'reg_lambda': 100}]\n",
      "best paras {'reg_lambda': 1}\n",
      "best_score_ -0.1277880892465591\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "#Step 5: Tuning Regularization Parameters\n",
    "##########################################\n",
    "\n",
    "'''\n",
    "lambda[default = 1]: L2 regularization term on weights, increase this value will make model more conservative.\n",
    "alphad[efault = 0]: L1 regularization term on weights, increase this value will make model more conservative.\n",
    "\n",
    "Theoretically, L1 is more unstable than L2.\n",
    "\n",
    "'''\n",
    "param_test7 = {\n",
    " 'reg_lambda':[1e-5, 1e-2, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "\n",
    "gsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, \n",
    "\tn_estimators = fixed_estimator, \n",
    "\tmax_depth= gsearch2.best_params_['max_depth'],\n",
    " \tmin_child_weight= gsearch2.best_params_['min_child_weight'], \n",
    " \tgamma= gsearch3.best_params_['gamma'], \n",
    " \tsubsample = 0.95, \n",
    " \tcolsample_bytree = 0.75,\n",
    " \tobjective= 'binary:logistic', \n",
    " \tnthread=4, \n",
    " \tscale_pos_weight=1, \n",
    " \tseed=seed), \n",
    " \tparam_grid = param_test7, \n",
    " \tscoring='neg_log_loss',\n",
    " \tn_jobs=4,\n",
    " \tcv=cv)\n",
    "# running\n",
    "gsearch7.fit(X_train, Y_train)\n",
    "\n",
    "print ('results',gsearch7.grid_scores_)\n",
    "print ('best paras',gsearch7.best_params_)\n",
    "print ('best_score_',gsearch7.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n"
     ]
    }
   ],
   "source": [
    "print (0.1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "#2. Lets go smaller interval centering around the above optimum.\n",
    "############################\n",
    "param_test8 = {\n",
    " 'reg_lambda':[0.1, 0.5, 1,10]\n",
    "}\n",
    "\n",
    "\n",
    "gsearch8 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, \n",
    "\tn_estimators = fixed_estimator, \n",
    "\tmax_depth= gsearch2.best_params_['max_depth'],\n",
    " \tmin_child_weight= gsearch2.best_params_['min_child_weight'], \n",
    " \tgamma= gsearch3.best_params_['gamma'], \n",
    " \tsubsample = 0.95, \n",
    " \tcolsample_bytree = 0.75,\n",
    " \tobjective= 'binary:logistic', \n",
    " \tnthread=4, \n",
    " \tscale_pos_weight=1, \n",
    " \tseed=seed,\n",
    "    reg_lambda=1), \n",
    " \tparam_grid = param_test8, \n",
    " \tscoring='neg_log_loss',\n",
    " \tn_jobs=4,\n",
    " \tcv=cv)\n",
    "# running\n",
    "gsearch8.fit(X_train, Y_train)\n",
    "\n",
    "print ('results',gsearch8.grid_scores_)\n",
    "print ('best paras',gsearch8.best_params_)\n",
    "print ('best_score_',gsearch8.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results [mean: -0.12789, std: 0.00063, params: {'reg_lambda': 0.1}, mean: -0.12788, std: 0.00065, params: {'reg_lambda': 0.5}, mean: -0.12779, std: 0.00062, params: {'reg_lambda': 1}, mean: -0.12778, std: 0.00065, params: {'reg_lambda': 10}]\n",
      "best paras {'reg_lambda': 10}\n",
      "best_score_ -0.1277819217394043\n"
     ]
    }
   ],
   "source": [
    "print ('results',gsearch8.grid_scores_)\n",
    "print ('best paras',gsearch8.best_params_)\n",
    "print ('best_score_',gsearch8.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results [mean: -0.12777, std: 0.00067, params: {'reg_lambda': 2.5}, mean: -0.12775, std: 0.00072, params: {'reg_lambda': 5}, mean: -0.12776, std: 0.00066, params: {'reg_lambda': 7.5}]\n",
      "best paras {'reg_lambda': 5}\n",
      "best_score_ -0.12774503424335962\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "#3. Lets go smaller interval centering around the above optimum all over again\n",
    "############################\n",
    "param_test9 = {\n",
    " 'reg_lambda':[2.5 , 5, 7.5]\n",
    "}\n",
    "\n",
    "\n",
    "gsearch9 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, \n",
    "\tn_estimators = fixed_estimator, \n",
    "\tmax_depth= gsearch2.best_params_['max_depth'],\n",
    " \tmin_child_weight= gsearch2.best_params_['min_child_weight'], \n",
    " \tgamma= gsearch3.best_params_['gamma'], \n",
    " \tsubsample = 0.95, \n",
    " \tcolsample_bytree = 0.75,\n",
    " \tobjective= 'binary:logistic', \n",
    " \tnthread=4, \n",
    " \tscale_pos_weight=1, \n",
    " \tseed=seed,\n",
    "    reg_lambda=1), \n",
    " \tparam_grid = param_test9, \n",
    " \tscoring='neg_log_loss',\n",
    " \tn_jobs=4,\n",
    " \tcv=cv)\n",
    "# running\n",
    "gsearch9.fit(X_train, Y_train)\n",
    "\n",
    "print ('results',gsearch9.grid_scores_)\n",
    "print ('best paras',gsearch9.best_params_)\n",
    "print ('best_score_',gsearch9.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2.best_params_['max_depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2.best_params_['min_child_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch3.best_params_['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
